# ðŸ§  MNIST Experiments with Energy-Based Models

This repository contains experiments on the **MNIST dataset** using various energy-based neural network models. Each model is trained independently and evaluated through its ability to reconstruct images and generate samples.

## ðŸ“Œ Models Implemented

The following networks were trained and analyzed:

- **Hopfield Network**  
  A classical recurrent network that stores patterns as stable states and retrieves them based on similarity.

- **Bernoulli Restricted Boltzmann Machine (RBM)**  
  A generative stochastic model with binary visible and hidden units, used to learn data distributions.

- **Bernoulli RBM with L1 Regularization**  
  An RBM extended with L1 penalty to promote sparsity in the learned representations.

- **Deep Boltzmann Machine (DBM)** with L1 Penalty  
  A multilayer generalization of RBMs, with L1 regularization applied to each layer for improved feature extraction and sparsity.

## ðŸ“Š Visualizations

For each model, we provide visual outputs including:

- **Fantasy Particles**  
  Samples generated from the model after training using Gibbs sampling.

- **Reconstructions**  
  Input images and their corresponding reconstructions generated by the trained model.

These visualizations help illustrate how well each model has captured the underlying structure of the MNIST digits.

---

## ðŸ“– A Brief Introduction to Restricted Boltzmann Machines (RBMs)

A **Restricted Boltzmann Machine (RBM)** is a generative stochastic neural network that learns a probability distribution over its inputs.

- Introduced as **Harmoniums** by Paul Smolensky (1986)
- Popularized by **Geoffrey Hinton** in the 2000s through fast training algorithms like Contrastive Divergence

### ðŸ”— Architecture

RBMs consist of:
- A **visible layer** representing the input data
- A **hidden layer** that captures latent features

The architecture is a **bipartite graph**, meaning:
- Connections exist only between visible and hidden units
- No intra-layer connections (i.e., no visible-visible or hidden-hidden links)

This structural restriction makes training more efficient compared to general Boltzmann Machines.

---

## ðŸš€ RBMs in Deep Learning

RBMs serve as building blocks for deeper models such as:

- **Deep Belief Networks (DBNs)**  
  Formed by stacking multiple RBMs and fine-tuning with backpropagation.

- **Deep Boltzmann Machines (DBMs)**  
  Fully generative deep models trained layer-by-layer with further Gibbs sampling steps.

RBMs and their variants have applications in:
- Dimensionality Reduction
- Feature Extraction
- Collaborative Filtering
- Topic Modeling
- Quantum Many-Body Physics

---

