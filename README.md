
This repository contains a series of experiments on the MNIST dataset using various energy-based models, implemented and analyzed in a Jupyter Notebook.

üß† Models Implemented
Each model was trained separately on the MNIST dataset:

Hopfield Network
A classical associative memory model used for pattern storage and retrieval.

Bernoulli Restricted Boltzmann Machine (RBM)
A probabilistic generative model that learns the data distribution using binary visible and hidden units.

Bernoulli RBM with L1 Regularization
Adds sparsity constraints via L1 regularization, encouraging sparse hidden representations.

Deep Boltzmann Machine (DBM)
A multi-layer extension of RBMs trained with an L1 penalty applied to each layer to promote sparsity and better generalization.

üîç Visualizations
The notebook includes visualizations for each model:

Fantasy Particles: Samples generated by the model during Gibbs sampling.

Reconstructions: Input images and their reconstructions to evaluate generative quality.

üßæ A Brief Introduction to Restricted Boltzmann Machines (RBMs)
A Restricted Boltzmann Machine (RBM) is a generative stochastic neural network that can learn a probability distribution over its input data. RBMs were originally introduced as Harmoniums by Paul Smolensky (1986), and later popularized by Geoffrey Hinton and collaborators in the early 2000s through fast training algorithms.

RBMs have been successfully applied to:

Dimensionality Reduction

Classification

Collaborative Filtering (e.g., Netflix Prize)

Feature Learning

Topic Modeling

Quantum Many-Body Systems

üîó Architecture
An RBM consists of two layers:

Visible Units: Represent observed data

Hidden Units: Capture latent features

These layers form a bipartite graph, meaning:

Connections exist only between visible and hidden units.

There are no intra-layer connections.

This restriction simplifies training and enables efficient approximation algorithms like Contrastive Divergence.

üß± RBMs in Deep Learning
RBMs can be stacked to form deeper architectures such as:

Deep Belief Networks (DBNs): Stacked RBMs followed by supervised fine-tuning.

Deep Boltzmann Machines (DBMs): Fully generative multi-layer models.
